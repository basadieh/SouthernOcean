{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Southern Ocean Codes\n",
    "## Environment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter some warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_gateway import Gateway\n",
    "\n",
    "gateway = Gateway()\n",
    "cluster = gateway.new_cluster()\n",
    "cluster.adapt(minimum = 2, maximum = 10)\n",
    "\n",
    "client = Client(cluster, timeout=\"50s\") \n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import gcsfs # Pythonic file-system for Google Cloud Storage\n",
    "import xesmf as xe\n",
    "import math\n",
    "import copy\n",
    "from scipy.interpolate import griddata\n",
    "# import seawater as sw\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# from mpl_toolkits.basemap import Basemap, cm, maskoceans\n",
    "\n",
    "# import os\n",
    "# os.environ['NUMPY_EXPERIMENTAL_ARRAY_FUNCTION'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Access and processing\n",
    "### 1. Fetch data and calculation\n",
    "If this process have been done and data have been saved before, this part doesn't need to be run again.\n",
    "Turn to the second part retrieve data from saved file.\n",
    "#### a) get data from gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_load_ds_uri(uri):\n",
    "    \"\"\"\n",
    "    Load data for given uri\n",
    "    \"\"\"\n",
    "    gcs = gcsfs.GCSFileSystem(token='anon') # GCSFS will attempt to use your default gcloud credentials\n",
    "    ds = xr.open_zarr(gcs.get_mapper(uri), consolidated=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCM_name = 'GFDL-CM4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt = df[(df.table_id == 'Omon') & \n",
    "            (df.variable_id == 'thetao') & \n",
    "            (df.activity_id == 'CMIP') & \n",
    "            (df.experiment_id == 'piControl') & \n",
    "            (df.source_id==GCM_name)]\n",
    "df_plt = df_plt[ df_plt['grid_label'] == 'gr']\n",
    "#run_counts = df_plt.groupby(['source_id', 'experiment_id'])['zstore'].count()\n",
    "#run_counts\n",
    "uri_thetao = df_plt[(df_plt.source_id == GCM_name)].zstore.values[0]\n",
    "uri_thetao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt = df[(df.table_id == 'Omon') & (df.variable_id == 'so') & (df.activity_id == 'CMIP') & (df.experiment_id == 'piControl')& (df.source_id==GCM_name)]\n",
    "df_plt = df_plt[ df_plt['grid_label'] == 'gr']\n",
    "uri_so = df_plt[(df_plt.source_id == GCM_name)].zstore.values[0]\n",
    "uri_so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_thetao = func_load_ds_uri(uri_thetao)\n",
    "ds_so = func_load_ds_uri(uri_so)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) select data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCM = GCM_name\n",
    "year_start=1\n",
    "year_end=500\n",
    "\n",
    "conv_index_depth = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_thetao = ds_thetao.isel(time=slice(year_start-1, year_end*12))\n",
    "ds_so = ds_so.isel(time=slice(year_start-1, year_end*12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Depths=ds_thetao.lev.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) calcalate MLD etc...\n",
    "    define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_MLD(dset_thetao, dset_so, month_no, hemis_no):\n",
    "    import seawater as sw\n",
    "    \n",
    "    yrs_no = np.int(len(ds_thetao.time)/12)\n",
    "    Depths=ds_thetao.lev.values\n",
    "\n",
    "    if hemis_no==0:\n",
    "        depth_MLD_tr=2000 # Weddell Sea - [30E-60W]\n",
    "    elif hemis_no==1:\n",
    "        depth_MLD_tr=1000 # Labrador Sea  - [60W-30W]\n",
    "    elif hemis_no==2:\n",
    "        depth_MLD_tr=2000 # Norwegian Sea\n",
    "    elif hemis_no==3:\n",
    "        depth_MLD_tr=1000 # Labrador Sea - Extended [60W-40W]  \n",
    "    elif hemis_no==4:\n",
    "        depth_MLD_tr=2000 # Ross Sea  - [160E-230E]           \n",
    "    else:\n",
    "        depth_MLD_tr=2000\n",
    "    deep_conv_area=[]\n",
    "    \n",
    "    Lon_orig=ds_thetao.lon.values\n",
    "    Lat_orig=ds_thetao.lat.values \n",
    "    if np.ndim(Lon_orig)==1: # If the GCM grid is not curvlinear\n",
    "        Lon_orig,Lat_orig=np.meshgrid(Lon_orig, Lat_orig)    \n",
    "        \n",
    "    lat_n=Lat_orig.shape[0] # Number of Lat elements in the data\n",
    "    lon_n=Lon_orig.shape[1] # Number of Lon elements in the data\n",
    "    earth_R = 6378e3 # Earth Radius - Unit is kilometer (km)\n",
    "    GridCell_Areas = np.zeros ((lat_n, lon_n )) # A = 2*pi*R^2 |sin(lat1)-sin(lat2)| |lon1-lon2|/360 = (pi/180)R^2 |lon1-lon2| |sin(lat1)-sin(lat2)| \n",
    "    for ii in range(1,lat_n-1):\n",
    "        for jj in range(1,lon_n-1):\n",
    "            GridCell_Areas [ii,jj] = math.fabs( (earth_R**2) * (math.pi/180) * np.absolute( (Lon_orig[ii,jj-1]+Lon_orig[ii,jj])/2  -  (Lon_orig[ii,jj]+Lon_orig[ii,jj+1])/2 )  * np.absolute( math.sin(math.radians( ( Lat_orig[ii-1,jj]+Lat_orig[ii,jj])/2 )) - math.sin(math.radians( Lat_orig[ii,jj]+Lat_orig[ii+1,jj])/2  )) )                  \n",
    "    for ii in range(1,lat_n-1):\n",
    "        for jj in range(2,lon_n-2):\n",
    "            if GridCell_Areas [ii,jj] > GridCell_Areas [ii,jj-1]*3:\n",
    "                GridCell_Areas [ii,jj]=GridCell_Areas [ii,jj-1]\n",
    "            if GridCell_Areas [ii,jj] > GridCell_Areas [ii,jj+1]*3:\n",
    "                GridCell_Areas [ii,jj]=GridCell_Areas [ii,jj+1]\n",
    "    GridCell_Areas[0,:]=GridCell_Areas[1,:]; GridCell_Areas[-1,:]=GridCell_Areas[-2,:]\n",
    "    GridCell_Areas[:,0]=GridCell_Areas[:,1]; GridCell_Areas[:,-1]=GridCell_Areas[:,-2]\n",
    "    areacello=GridCell_Areas      \n",
    "    \n",
    "    lat_n_regrid=90\n",
    "    lon_n_regrid=180\n",
    "    Lat_regrid_1D, Lon_regrid_1D, Lat_bound_regrid, Lon_bound_regrid = func_latlon_regrid_eq(lat_n_regrid, lon_n_regrid, -90, 90, 0, 360)\n",
    "    lon, lat = np.meshgrid(Lon_regrid_1D, Lat_regrid_1D)\n",
    "    areacello = func_regrid(areacello, Lat_orig, Lon_orig, lat, lon)\n",
    "    \n",
    "    # ds_so.so.isel(lev=slice(0,10))   # slice(start, end, step)\n",
    "    \n",
    "    data_plot=np.full([yrs_no,len(lon),len(lon[0])], np.nan)    \n",
    "    \n",
    "    for t in tqdm(range(yrs_no)):\n",
    "        #print('MLD calc - Year: ', t+1)\n",
    "        data_thetao_extracted = ds_thetao.thetao.isel(time= 12*t+month_no-1 ).values\n",
    "        data_so_extracted = ds_so.so.isel(time= 12*t+month_no-1 ).values\n",
    "        data_dens=sw.dens0(data_so_extracted, data_thetao_extracted)\n",
    "        depth10m_shalow=0\n",
    "        depth10m_deep=0\n",
    "        depth_array=np.asarray(Depths)\n",
    "        \n",
    "        for k in range(len(Depths)):\n",
    "            if Depths[k]<=10:\n",
    "                depth10m_shalow=k\n",
    "        for k in range(len(Depths)):        \n",
    "            if Depths[k]>=10:\n",
    "                depth10m_deep+=k\n",
    "                break\n",
    "                \n",
    "        interpol_x = [Depths[depth10m_shalow], Depths[depth10m_deep]]\n",
    "        data_i=data_dens\n",
    "        data_i = func_regrid(data_dens, Lat_orig, Lon_orig, lat, lon)\n",
    "        data_i[data_i>100000]=np.nan\n",
    "        \n",
    "        if (int(hemis_no)==int(0)):# Weddell Sea\n",
    "            [ii,jj] = np.where(lat<=-50)###indeces####\n",
    "        elif (int(hemis_no)==int(1)):# Labrador Sea  - [60W-30W]\n",
    "            [ii,jj] = np.where(lat>=50)###indeces####\n",
    "        elif (int(hemis_no)==int(2)):# Norwegian Sea\n",
    "            [ii,jj] = np.where(lat>=58)###indeces####\n",
    "        elif (int(hemis_no)==int(3)):# Labrador Sea  - [60W-40W]  \n",
    "            [ii,jj] = np.where(lat>=50)###indeces####   \n",
    "        elif (int(hemis_no)==int(4)):# Ross Sea  - [160E-230E]   \n",
    "            [ii,jj] = np.where(lat<=-50)###indeces####             \n",
    "        else:\n",
    "            print(hemis_no)\n",
    "            print('invalid input for hemisphere option')\n",
    "            break                \n",
    "\n",
    "        area=0\n",
    "        for k in range(len(ii)):\n",
    "            if not(str(data_i[0,ii[k],jj[k]])=='nan'):\n",
    "                dummy=100\n",
    "                interpol_dens = [data_i[depth10m_shalow,ii[k],jj[k]], data_i[depth10m_deep,ii[k],jj[k]]]\n",
    "                p_10m_dens = np.interp(10, interpol_x, interpol_dens)\n",
    "                for d in range(len(data_i)):\n",
    "                    if not(str(data_i[0,ii[k],jj[k]])=='nan'):\n",
    "                        p_dens = data_i[d,ii[k],jj[k]]\n",
    "                        if abs(p_dens-p_10m_dens-0.03)<dummy:\n",
    "                            dummy=abs(p_dens-p_10m_dens-0.03)\n",
    "                            MLD=d\n",
    "                if MLD==0:\n",
    "                    MLD+=1\n",
    "                    p_dens_interpol = [data_i[MLD-1,ii[k],jj[k]]-p_10m_dens,data_i[MLD,ii[k],jj[k]]-p_10m_dens,data_i[MLD+1,ii[k],jj[k]]-p_10m_dens]\n",
    "                    depth_levels = [depth_array[MLD-1],depth_array[MLD],depth_array[MLD+1]]\n",
    "                ##elif MLD==49:\n",
    "                elif MLD==len(data_i)-1: # If MLD is the last layer                   \n",
    "                    MLD-=1\n",
    "                    p_dens_interpol = [data_i[MLD-1,ii[k],jj[k]]-p_10m_dens,data_i[MLD,ii[k],jj[k]]-p_10m_dens,data_i[MLD+1,ii[k],jj[k]]-p_10m_dens]\n",
    "                    depth_levels = [depth_array[MLD-1],depth_array[MLD],depth_array[MLD+1]]\n",
    "                else:\n",
    "                    p_dens_interpol = [data_i[MLD-1,ii[k],jj[k]]-p_10m_dens,data_i[MLD,ii[k],jj[k]]-p_10m_dens,data_i[MLD+1,ii[k],jj[k]]-p_10m_dens]\n",
    "                    depth_levels = [depth_array[MLD-1],depth_array[MLD],depth_array[MLD+1]]\n",
    "                interpol_z=np.interp(0.03,p_dens_interpol,depth_levels)\n",
    "                if interpol_z>=depth_MLD_tr:\n",
    "                #y1+=float(interpol_z)\n",
    "                    area+=areacello[ii[k],jj[k]]\n",
    "                    data_plot[t,ii[k],jj[k]]=float(interpol_z)     \n",
    "        deep_conv_area.append(area)\n",
    "    deep_conv_area=np.asarray(deep_conv_area)   \n",
    "    \n",
    "    average_MLD=np.nanmean(data_plot,axis=0)\n",
    "    if hemis_no==0: # SH, Weddell Sea\n",
    "        indeces = np.where(np.logical_or((lon<=30) & (average_MLD>depth_MLD_tr), (lon>=300) &(average_MLD>depth_MLD_tr)))\n",
    "    elif hemis_no==1: # NH, Labrador Sea  - [60W-30W]\n",
    "        indeces = np.where(np.logical_and((lon>=30) & (average_MLD>depth_MLD_tr), (lon<=330) &(average_MLD>depth_MLD_tr)))\n",
    "    elif hemis_no==2: # NH, Norwegian Sea\n",
    "        indeces = np.where(np.logical_or((lon<=30) & (average_MLD>depth_MLD_tr), (lon>=345) &(average_MLD>depth_MLD_tr)))\n",
    "    elif hemis_no==3: # NH, Labrador Sea  - [60W-40W]\n",
    "        indeces = np.where(np.logical_and((lon>=30) & (average_MLD>depth_MLD_tr), (lon<=320) &(average_MLD>depth_MLD_tr)))\n",
    "    elif hemis_no==4: # NH, Ross Sea  - [160E-230E] \n",
    "        indeces = np.where(np.logical_and((lon>=160) & (average_MLD>depth_MLD_tr), (lon<=230) &(average_MLD>depth_MLD_tr)))        \n",
    "    else: ### This should never be the case though ###\n",
    "        indeces = np.where(np.logical_and((lon>=30) & (average_MLD>depth_MLD_tr), (lon<=330) &(average_MLD>depth_MLD_tr)))        \n",
    "        \n",
    "    return deep_conv_area, data_plot, lon, lat, indeces     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_time_depth_plot(ds_thetao, indeces, conv_index_depth):\n",
    "\n",
    "    Depths=ds_thetao.lev.values\n",
    "    yrs_no = np.int(len(ds_thetao.time)/12)\n",
    "    Lon_orig=ds_thetao.lon.values\n",
    "    Lat_orig=ds_thetao.lat.values    \n",
    "    \n",
    "    [ii,jj]=indeces\n",
    "    region=[]\n",
    "    \n",
    "    lat_n_regrid=90\n",
    "    lon_n_regrid=180\n",
    "    Lat_regrid_1D, Lon_regrid_1D, Lat_bound_regrid, Lon_bound_regrid = func_latlon_regrid_eq(lat_n_regrid, lon_n_regrid, -90, 90, 0, 360)\n",
    "    lon, lat = np.meshgrid(Lon_regrid_1D, Lat_regrid_1D)     \n",
    "    \n",
    "    for t in tqdm(range(yrs_no)):\n",
    "        data = ds_thetao.thetao.isel(time= slice(12*t,12*t+11) ).values\n",
    "        data=np.asarray(data)\n",
    "        #data[data>100000]=np.nan\n",
    "        data=np.nanmean(data,axis=0)\n",
    "        data=np.squeeze(data)\n",
    "        data_i = func_regrid(data, Lat_orig, Lon_orig, lat, lon)\n",
    "        data_i=data_i[:,ii,jj]\n",
    "        \n",
    "        #print('time_depth_plot calc - Year: ', t+1)\n",
    "        region.append(np.nanmean(data_i,axis=1))\n",
    "\n",
    "    depth_index_start=0\n",
    "    depth_index_end=0\n",
    "    if conv_index_depth==0:\n",
    "        depth_index_start=0\n",
    "        depth_index_end=1\n",
    "    else:\n",
    "        for i in range(len(Depths[:])):\n",
    "            if Depths[i]<=conv_index_depth:\n",
    "                depth_index_start=i\n",
    "        for i in range(len(Depths[:])):\n",
    "            if Depths[i]<=conv_index_depth:\n",
    "                depth_index_end=i\n",
    "    if depth_index_end==0:\n",
    "        depth_index_end+=1        \n",
    "        \n",
    "        \n",
    "    print(depth_index_start)\n",
    "    region=np.asarray(region)\n",
    "    convection_index=region[:,depth_index_start]\n",
    "    return region,convection_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added by Grace, adapting Behzad's code to get average salinity over convection area\n",
    "def func_time_depth_plot_so(ds, indeces, conv_index_depth):\n",
    "\n",
    "    Depths=ds.lev.values\n",
    "    yrs_no = np.int(len(ds.time)/12)\n",
    "    Lon_orig=ds.lon.values\n",
    "    Lat_orig=ds.lat.values    \n",
    "    \n",
    "    [ii,jj]=indeces\n",
    "    region=[]\n",
    "    \n",
    "    lat_n_regrid=90\n",
    "    lon_n_regrid=180\n",
    "    Lat_regrid_1D, Lon_regrid_1D, Lat_bound_regrid, Lon_bound_regrid = func_latlon_regrid_eq(lat_n_regrid, lon_n_regrid, -90, 90, 0, 360)\n",
    "    lon, lat = np.meshgrid(Lon_regrid_1D, Lat_regrid_1D)     \n",
    "    \n",
    "    for t in tqdm(range(yrs_no)):\n",
    "        data = ds.so.isel(time= slice(12*t,12*t+11) ).values\n",
    "        data=np.asarray(data)\n",
    "        #data[data>100000]=np.nan\n",
    "        data=np.nanmean(data,axis=0)\n",
    "        data=np.squeeze(data)\n",
    "        data_i = func_regrid(data, Lat_orig, Lon_orig, lat, lon)\n",
    "        data_i=data_i[:,ii,jj]\n",
    "        \n",
    "        #print('time_depth_plot calc - Year: ', t+1)\n",
    "        region.append(np.nanmean(data_i,axis=1))\n",
    "\n",
    "    depth_index_start=0\n",
    "    depth_index_end=0\n",
    "    if conv_index_depth==0:\n",
    "        depth_index_start=0\n",
    "        depth_index_end=1\n",
    "    else:\n",
    "        for i in range(len(Depths[:])):\n",
    "            if Depths[i]<=conv_index_depth:\n",
    "                depth_index_start=i\n",
    "        for i in range(len(Depths[:])):\n",
    "            if Depths[i]<=conv_index_depth:\n",
    "                depth_index_end=i\n",
    "    if depth_index_end==0:\n",
    "        depth_index_end+=1        \n",
    "        \n",
    "        \n",
    "    print(depth_index_start)\n",
    "    region=np.asarray(region)\n",
    "    convection_index=region[:,depth_index_start]\n",
    "    return region,convection_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Density_allyears = []\n",
    "yrs_no = np.int(len(ds_thetao.time)/12)\n",
    "Lon_orig=ds_thetao.lon.values\n",
    "Lat_orig=ds_thetao.lat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seawater as sw\n",
    "for t in tqdm(range(yrs_no)):\n",
    "    data_thetao_extracted = ds_thetao.thetao.isel(time= slice(12*t,12*t+12) ).values\n",
    "    data_so_extracted = ds_so.so.isel(time= slice(12*t,12*t+12) ).values\n",
    "    data_dens=sw.dens0(data_so_extracted, data_thetao_extracted)\n",
    "    data_dens = np.nanmean(data_dens, axis=0)\n",
    "    data_i = func_regrid(data_dens, Lat_orig, Lon_orig, Lat_regrid_2D, Lon_regrid_2D)\n",
    "    data_i[np.abs(data_i)>1e16]=np.nan # converting 1e+20 to nan\n",
    "    \n",
    "    Density_allyears.append(data_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Density_allyears = np.asarray(Density_allyears)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Density_allyears_GFDL-CM4_500yr.npy', Density_allyears)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. restore data from saved file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shelve\n",
    "\n",
    "dir_pwd = os.getcwd() # Gets the current directory (and in which the code is placed)\n",
    "filename_out = (dir_pwd + '/AllResults_'+GCM+'_500yr.out') # Directory to save processed data\n",
    "my_shelf = shelve.open(filename_out)\n",
    "for key in my_shelf:\n",
    "    globals()[key]=my_shelf[key]\n",
    "my_shelf.close()\n",
    "\n",
    "filename_out = (dir_pwd + '/AllResults_'+GCM+'_500yr_ROSS.out') # Directory to save processed data\n",
    "my_shelf = shelve.open(filename_out)\n",
    "for key in my_shelf:\n",
    "    globals()[key]=my_shelf[key]\n",
    "my_shelf.close()\n",
    "\n",
    "filename_out = (dir_pwd + '/AllResults_'+GCM+'_500yr_WS.out') # Directory to save processed data\n",
    "my_shelf = shelve.open(filename_out)\n",
    "for key in my_shelf:\n",
    "    globals()[key]=my_shelf[key]\n",
    "my_shelf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
