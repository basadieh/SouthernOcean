{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Southern Ocean Codes\n",
    "## Environment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter some warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "#from dask_kubernetes import KubeCluster\n",
    "from dask_gateway import Gateway\n",
    "\n",
    "#cluster = KubeCluster() \n",
    "gateway = Gateway()\n",
    "cluster = gateway.new_cluster()\n",
    "cluster.adapt(minimum = 1, maximum = 10)\n",
    "\n",
    "client = Client(cluster, timeout=\"50s\") \n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade git+https://github.com/JiaweiZhuang/xESMF.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import gcsfs # Pythonic file-system for Google Cloud Storage\n",
    "import xesmf as xe\n",
    "import math\n",
    "import copy\n",
    "from scipy.interpolate import griddata\n",
    "# import seawater as sw\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# from mpl_toolkits.basemap import Basemap, cm, maskoceans\n",
    "\n",
    "# import os\n",
    "# os.environ['NUMPY_EXPERIMENTAL_ARRAY_FUNCTION'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Access and processing\n",
    "### 1. Fetch data and calculation\n",
    "If this process have been done and data have been saved before, this part doesn't need to be run again.\n",
    "Turn to the second part retrieve data from saved file.\n",
    "#### a) get data from gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_load_ds_uri(uri):\n",
    "    \"\"\"\n",
    "    Load data for given uri\n",
    "    \"\"\"\n",
    "    gcs = gcsfs.GCSFileSystem(token='anon') # GCSFS will attempt to use your default gcloud credentials\n",
    "    ds = xr.open_zarr(gcs.get_mapper(uri), consolidated=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCM_name = 'GFDL-CM4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt = df[(df.table_id == 'Omon') & \n",
    "            (df.variable_id == 'thetao') & \n",
    "            (df.activity_id == 'CMIP') & \n",
    "            (df.experiment_id == 'piControl') & \n",
    "            (df.source_id==GCM_name)]\n",
    "df_plt = df_plt[ df_plt['grid_label'] == 'gr']\n",
    "#run_counts = df_plt.groupby(['source_id', 'experiment_id'])['zstore'].count()\n",
    "#run_counts\n",
    "uri_thetao = df_plt[(df_plt.source_id == GCM_name)].zstore.values[0]\n",
    "uri_thetao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt = df[(df.table_id == 'Omon') & (df.variable_id == 'so') & (df.activity_id == 'CMIP') & (df.experiment_id == 'piControl')& (df.source_id==GCM_name)]\n",
    "df_plt = df_plt[ df_plt['grid_label'] == 'gr']\n",
    "uri_so = df_plt[(df_plt.source_id == GCM_name)].zstore.values[0]\n",
    "uri_so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_thetao = func_load_ds_uri(uri_thetao)\n",
    "ds_so = func_load_ds_uri(uri_so)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) select data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCM = GCM_name\n",
    "year_start=1\n",
    "year_end=500\n",
    "\n",
    "conv_index_depth = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_thetao = ds_thetao.isel(time=slice(year_start-1, year_end*12))\n",
    "ds_so = ds_so.isel(time=slice(year_start-1, year_end*12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Depths=ds_thetao.lev.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) calcalate MLD etc...\n",
    "    define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_MLD(dset_thetao, dset_so, month_no, hemis_no):\n",
    "    import seawater as sw\n",
    "    \n",
    "    yrs_no = np.int(len(ds_thetao.time)/12)\n",
    "    Depths=ds_thetao.lev.values\n",
    "\n",
    "    if hemis_no==0:\n",
    "        depth_MLD_tr=2000 # Weddell Sea - [30E-60W]\n",
    "    elif hemis_no==1:\n",
    "        depth_MLD_tr=1000 # Labrador Sea  - [60W-30W]\n",
    "    elif hemis_no==2:\n",
    "        depth_MLD_tr=2000 # Norwegian Sea\n",
    "    elif hemis_no==3:\n",
    "        depth_MLD_tr=1000 # Labrador Sea - Extended [60W-40W]  \n",
    "    elif hemis_no==4:\n",
    "        depth_MLD_tr=2000 # Ross Sea  - [160E-230E]           \n",
    "    else:\n",
    "        depth_MLD_tr=2000\n",
    "    deep_conv_area=[]\n",
    "    \n",
    "    Lon_orig=ds_thetao.lon.values\n",
    "    Lat_orig=ds_thetao.lat.values \n",
    "    if np.ndim(Lon_orig)==1: # If the GCM grid is not curvlinear\n",
    "        Lon_orig,Lat_orig=np.meshgrid(Lon_orig, Lat_orig)    \n",
    "        \n",
    "    lat_n=Lat_orig.shape[0] # Number of Lat elements in the data\n",
    "    lon_n=Lon_orig.shape[1] # Number of Lon elements in the data\n",
    "    earth_R = 6378e3 # Earth Radius - Unit is kilometer (km)\n",
    "    GridCell_Areas = np.zeros ((lat_n, lon_n )) # A = 2*pi*R^2 |sin(lat1)-sin(lat2)| |lon1-lon2|/360 = (pi/180)R^2 |lon1-lon2| |sin(lat1)-sin(lat2)| \n",
    "    for ii in range(1,lat_n-1):\n",
    "        for jj in range(1,lon_n-1):\n",
    "            GridCell_Areas [ii,jj] = math.fabs( (earth_R**2) * (math.pi/180) * np.absolute( (Lon_orig[ii,jj-1]+Lon_orig[ii,jj])/2  -  (Lon_orig[ii,jj]+Lon_orig[ii,jj+1])/2 )  * np.absolute( math.sin(math.radians( ( Lat_orig[ii-1,jj]+Lat_orig[ii,jj])/2 )) - math.sin(math.radians( Lat_orig[ii,jj]+Lat_orig[ii+1,jj])/2  )) )                  \n",
    "    for ii in range(1,lat_n-1):\n",
    "        for jj in range(2,lon_n-2):\n",
    "            if GridCell_Areas [ii,jj] > GridCell_Areas [ii,jj-1]*3:\n",
    "                GridCell_Areas [ii,jj]=GridCell_Areas [ii,jj-1]\n",
    "            if GridCell_Areas [ii,jj] > GridCell_Areas [ii,jj+1]*3:\n",
    "                GridCell_Areas [ii,jj]=GridCell_Areas [ii,jj+1]\n",
    "    GridCell_Areas[0,:]=GridCell_Areas[1,:]; GridCell_Areas[-1,:]=GridCell_Areas[-2,:]\n",
    "    GridCell_Areas[:,0]=GridCell_Areas[:,1]; GridCell_Areas[:,-1]=GridCell_Areas[:,-2]\n",
    "    areacello=GridCell_Areas      \n",
    "    \n",
    "    lat_n_regrid=90\n",
    "    lon_n_regrid=180\n",
    "    Lat_regrid_1D, Lon_regrid_1D, Lat_bound_regrid, Lon_bound_regrid = func_latlon_regrid_eq(lat_n_regrid, lon_n_regrid, -90, 90, 0, 360)\n",
    "    lon, lat = np.meshgrid(Lon_regrid_1D, Lat_regrid_1D)\n",
    "    areacello = func_regrid(areacello, Lat_orig, Lon_orig, lat, lon)\n",
    "    \n",
    "    # ds_so.so.isel(lev=slice(0,10))   # slice(start, end, step)\n",
    "    \n",
    "    data_plot=np.full([yrs_no,len(lon),len(lon[0])], np.nan)    \n",
    "    \n",
    "    for t in tqdm(range(yrs_no)):\n",
    "        #print('MLD calc - Year: ', t+1)\n",
    "        data_thetao_extracted = ds_thetao.thetao.isel(time= 12*t+month_no-1 ).values\n",
    "        data_so_extracted = ds_so.so.isel(time= 12*t+month_no-1 ).values\n",
    "        data_dens=sw.dens0(data_so_extracted, data_thetao_extracted)\n",
    "        depth10m_shalow=0\n",
    "        depth10m_deep=0\n",
    "        depth_array=np.asarray(Depths)\n",
    "        \n",
    "        for k in range(len(Depths)):\n",
    "            if Depths[k]<=10:\n",
    "                depth10m_shalow=k\n",
    "        for k in range(len(Depths)):        \n",
    "            if Depths[k]>=10:\n",
    "                depth10m_deep+=k\n",
    "                break\n",
    "                \n",
    "        interpol_x = [Depths[depth10m_shalow], Depths[depth10m_deep]]\n",
    "        data_i=data_dens\n",
    "        data_i = func_regrid(data_dens, Lat_orig, Lon_orig, lat, lon)\n",
    "        data_i[data_i>100000]=np.nan\n",
    "        \n",
    "        if (int(hemis_no)==int(0)):# Weddell Sea\n",
    "            [ii,jj] = np.where(lat<=-50)###indeces####\n",
    "        elif (int(hemis_no)==int(1)):# Labrador Sea  - [60W-30W]\n",
    "            [ii,jj] = np.where(lat>=50)###indeces####\n",
    "        elif (int(hemis_no)==int(2)):# Norwegian Sea\n",
    "            [ii,jj] = np.where(lat>=58)###indeces####\n",
    "        elif (int(hemis_no)==int(3)):# Labrador Sea  - [60W-40W]  \n",
    "            [ii,jj] = np.where(lat>=50)###indeces####   \n",
    "        elif (int(hemis_no)==int(4)):# Ross Sea  - [160E-230E]   \n",
    "            [ii,jj] = np.where(lat<=-50)###indeces####             \n",
    "        else:\n",
    "            print(hemis_no)\n",
    "            print('invalid input for hemisphere option')\n",
    "            break                \n",
    "\n",
    "        area=0\n",
    "        for k in range(len(ii)):\n",
    "            if not(str(data_i[0,ii[k],jj[k]])=='nan'):\n",
    "                dummy=100\n",
    "                interpol_dens = [data_i[depth10m_shalow,ii[k],jj[k]], data_i[depth10m_deep,ii[k],jj[k]]]\n",
    "                p_10m_dens = np.interp(10, interpol_x, interpol_dens)\n",
    "                for d in range(len(data_i)):\n",
    "                    if not(str(data_i[0,ii[k],jj[k]])=='nan'):\n",
    "                        p_dens = data_i[d,ii[k],jj[k]]\n",
    "                        if abs(p_dens-p_10m_dens-0.03)<dummy:\n",
    "                            dummy=abs(p_dens-p_10m_dens-0.03)\n",
    "                            MLD=d\n",
    "                if MLD==0:\n",
    "                    MLD+=1\n",
    "                    p_dens_interpol = [data_i[MLD-1,ii[k],jj[k]]-p_10m_dens,data_i[MLD,ii[k],jj[k]]-p_10m_dens,data_i[MLD+1,ii[k],jj[k]]-p_10m_dens]\n",
    "                    depth_levels = [depth_array[MLD-1],depth_array[MLD],depth_array[MLD+1]]\n",
    "                ##elif MLD==49:\n",
    "                elif MLD==len(data_i)-1: # If MLD is the last layer                   \n",
    "                    MLD-=1\n",
    "                    p_dens_interpol = [data_i[MLD-1,ii[k],jj[k]]-p_10m_dens,data_i[MLD,ii[k],jj[k]]-p_10m_dens,data_i[MLD+1,ii[k],jj[k]]-p_10m_dens]\n",
    "                    depth_levels = [depth_array[MLD-1],depth_array[MLD],depth_array[MLD+1]]\n",
    "                else:\n",
    "                    p_dens_interpol = [data_i[MLD-1,ii[k],jj[k]]-p_10m_dens,data_i[MLD,ii[k],jj[k]]-p_10m_dens,data_i[MLD+1,ii[k],jj[k]]-p_10m_dens]\n",
    "                    depth_levels = [depth_array[MLD-1],depth_array[MLD],depth_array[MLD+1]]\n",
    "                interpol_z=np.interp(0.03,p_dens_interpol,depth_levels)\n",
    "                if interpol_z>=depth_MLD_tr:\n",
    "                #y1+=float(interpol_z)\n",
    "                    area+=areacello[ii[k],jj[k]]\n",
    "                    data_plot[t,ii[k],jj[k]]=float(interpol_z)     \n",
    "        deep_conv_area.append(area)\n",
    "    deep_conv_area=np.asarray(deep_conv_area)   \n",
    "    \n",
    "    average_MLD=np.nanmean(data_plot,axis=0)\n",
    "    if hemis_no==0: # SH, Weddell Sea\n",
    "        indeces = np.where(np.logical_or((lon<=30) & (average_MLD>depth_MLD_tr), (lon>=300) &(average_MLD>depth_MLD_tr)))\n",
    "    elif hemis_no==1: # NH, Labrador Sea  - [60W-30W]\n",
    "        indeces = np.where(np.logical_and((lon>=30) & (average_MLD>depth_MLD_tr), (lon<=330) &(average_MLD>depth_MLD_tr)))\n",
    "    elif hemis_no==2: # NH, Norwegian Sea\n",
    "        indeces = np.where(np.logical_or((lon<=30) & (average_MLD>depth_MLD_tr), (lon>=345) &(average_MLD>depth_MLD_tr)))\n",
    "    elif hemis_no==3: # NH, Labrador Sea  - [60W-40W]\n",
    "        indeces = np.where(np.logical_and((lon>=30) & (average_MLD>depth_MLD_tr), (lon<=320) &(average_MLD>depth_MLD_tr)))\n",
    "    elif hemis_no==4: # NH, Ross Sea  - [160E-230E] \n",
    "        indeces = np.where(np.logical_and((lon>=160) & (average_MLD>depth_MLD_tr), (lon<=230) &(average_MLD>depth_MLD_tr)))        \n",
    "    else: ### This should never be the case though ###\n",
    "        indeces = np.where(np.logical_and((lon>=30) & (average_MLD>depth_MLD_tr), (lon<=330) &(average_MLD>depth_MLD_tr)))        \n",
    "        \n",
    "    return deep_conv_area, data_plot, lon, lat, indeces     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_time_depth_plot(ds_thetao, indeces, conv_index_depth):\n",
    "\n",
    "    Depths=ds_thetao.lev.values\n",
    "    yrs_no = np.int(len(ds_thetao.time)/12)\n",
    "    Lon_orig=ds_thetao.lon.values\n",
    "    Lat_orig=ds_thetao.lat.values    \n",
    "    \n",
    "    [ii,jj]=indeces\n",
    "    region=[]\n",
    "    \n",
    "    lat_n_regrid=90\n",
    "    lon_n_regrid=180\n",
    "    Lat_regrid_1D, Lon_regrid_1D, Lat_bound_regrid, Lon_bound_regrid = func_latlon_regrid_eq(lat_n_regrid, lon_n_regrid, -90, 90, 0, 360)\n",
    "    lon, lat = np.meshgrid(Lon_regrid_1D, Lat_regrid_1D)     \n",
    "    \n",
    "    for t in tqdm(range(yrs_no)):\n",
    "        data = ds_thetao.thetao.isel(time= slice(12*t,12*t+11) ).values\n",
    "        data=np.asarray(data)\n",
    "        #data[data>100000]=np.nan\n",
    "        data=np.nanmean(data,axis=0)\n",
    "        data=np.squeeze(data)\n",
    "        data_i = func_regrid(data, Lat_orig, Lon_orig, lat, lon)\n",
    "        data_i=data_i[:,ii,jj]\n",
    "        \n",
    "        #print('time_depth_plot calc - Year: ', t+1)\n",
    "        region.append(np.nanmean(data_i,axis=1))\n",
    "\n",
    "    depth_index_start=0\n",
    "    depth_index_end=0\n",
    "    if conv_index_depth==0:\n",
    "        depth_index_start=0\n",
    "        depth_index_end=1\n",
    "    else:\n",
    "        for i in range(len(Depths[:])):\n",
    "            if Depths[i]<=conv_index_depth:\n",
    "                depth_index_start=i\n",
    "        for i in range(len(Depths[:])):\n",
    "            if Depths[i]<=conv_index_depth:\n",
    "                depth_index_end=i\n",
    "    if depth_index_end==0:\n",
    "        depth_index_end+=1        \n",
    "        \n",
    "        \n",
    "    print(depth_index_start)\n",
    "    region=np.asarray(region)\n",
    "    convection_index=region[:,depth_index_start]\n",
    "    return region,convection_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added by Grace, adapting Behzad's code to get average salinity over convection area\n",
    "def func_time_depth_plot_so(ds, indeces, conv_index_depth):\n",
    "\n",
    "    Depths=ds.lev.values\n",
    "    yrs_no = np.int(len(ds.time)/12)\n",
    "    Lon_orig=ds.lon.values\n",
    "    Lat_orig=ds.lat.values    \n",
    "    \n",
    "    [ii,jj]=indeces\n",
    "    region=[]\n",
    "    \n",
    "    lat_n_regrid=90\n",
    "    lon_n_regrid=180\n",
    "    Lat_regrid_1D, Lon_regrid_1D, Lat_bound_regrid, Lon_bound_regrid = func_latlon_regrid_eq(lat_n_regrid, lon_n_regrid, -90, 90, 0, 360)\n",
    "    lon, lat = np.meshgrid(Lon_regrid_1D, Lat_regrid_1D)     \n",
    "    \n",
    "    for t in tqdm(range(yrs_no)):\n",
    "        data = ds.so.isel(time= slice(12*t,12*t+11) ).values\n",
    "        data=np.asarray(data)\n",
    "        #data[data>100000]=np.nan\n",
    "        data=np.nanmean(data,axis=0)\n",
    "        data=np.squeeze(data)\n",
    "        data_i = func_regrid(data, Lat_orig, Lon_orig, lat, lon)\n",
    "        data_i=data_i[:,ii,jj]\n",
    "        \n",
    "        #print('time_depth_plot calc - Year: ', t+1)\n",
    "        region.append(np.nanmean(data_i,axis=1))\n",
    "\n",
    "    depth_index_start=0\n",
    "    depth_index_end=0\n",
    "    if conv_index_depth==0:\n",
    "        depth_index_start=0\n",
    "        depth_index_end=1\n",
    "    else:\n",
    "        for i in range(len(Depths[:])):\n",
    "            if Depths[i]<=conv_index_depth:\n",
    "                depth_index_start=i\n",
    "        for i in range(len(Depths[:])):\n",
    "            if Depths[i]<=conv_index_depth:\n",
    "                depth_index_end=i\n",
    "    if depth_index_end==0:\n",
    "        depth_index_end+=1        \n",
    "        \n",
    "        \n",
    "    print(depth_index_start)\n",
    "    region=np.asarray(region)\n",
    "    convection_index=region[:,depth_index_start]\n",
    "    return region,convection_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. restore data from saved file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shelve\n",
    "\n",
    "dir_pwd = os.getcwd() # Gets the current directory (and in which the code is placed)\n",
    "filename_out = (dir_pwd + '/AllResults_'+GCM+'_500yr.out') # Directory to save processed data\n",
    "my_shelf = shelve.open(filename_out)\n",
    "for key in my_shelf:\n",
    "    globals()[key]=my_shelf[key]\n",
    "my_shelf.close()\n",
    "\n",
    "filename_out = (dir_pwd + '/AllResults_'+GCM+'_500yr_ROSS.out') # Directory to save processed data\n",
    "my_shelf = shelve.open(filename_out)\n",
    "for key in my_shelf:\n",
    "    globals()[key]=my_shelf[key]\n",
    "my_shelf.close()\n",
    "\n",
    "filename_out = (dir_pwd + '/AllResults_'+GCM+'_500yr_WS.out') # Directory to save processed data\n",
    "my_shelf = shelve.open(filename_out)\n",
    "for key in my_shelf:\n",
    "    globals()[key]=my_shelf[key]\n",
    "my_shelf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
